{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for data cleaning the dataset with LLama2\n",
    "\n",
    "The following columns will be cleaned\n",
    "1. Maximum Height\n",
    "2. Flower Colour\n",
    "3. Trunk Texture\n",
    "4. Trunk Colour\n",
    "5. Leaf Texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and setting up of environment variables\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "os.environ['HG_ACCESS_TOKEN'] = '' # To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up\n",
    "Download Llama 2 into local system, ignore this cell if you already have a HG_ACCESS_TOKEN and do not want to download the models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llama(model_directory:str='../src/llama/model', tokenizer_directory:str='../src/llama/tokenizer'):\n",
    "    \"\"\"\n",
    "    Function to download llama2 7b model\n",
    "\n",
    "    Args:\n",
    "        model_directory (str): directory to the path with llama model, defaults to ../src/llama/model'\n",
    "        tokenizer_directory (str): directory to the path with llama tokenizer, defaults to ../src/llama/tokenizer'\n",
    "\n",
    "    Returns:\n",
    "        0 for success and 1 for failure\n",
    "    \"\"\"\n",
    "    hg_access_token = os.getenv('HG_ACCESS_TOKEN')\n",
    "    if len(hg_access_token) <=0:\n",
    "        print(\"No valid access key found, did you update your .env file?\")\n",
    "        return 1\n",
    "\n",
    "    if not os.path.isdir(model_directory):\n",
    "        os.makedirs(model_directory) \n",
    "        print(\"Llama Model directory not found, directory created\")\n",
    "\n",
    "    if not os.path.isdir(tokenizer_directory):\n",
    "        os.makedirs(tokenizer_directory) \n",
    "        print(\"Llama tokenizer directory not found, directory created\")\n",
    "\n",
    "    print(\"Preparing to download LLama\")\n",
    "\n",
    "    try:\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access_token)\n",
    "        llm_model.save_pretrained(model_directory)\n",
    "        print(\"LLama Model downloaded\")\n",
    "\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access_token)\n",
    "        llm_tokenizer.save_pretrained(tokenizer_directory)\n",
    "        print(\"LLama Tokenizer downloaded\")\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download Llama, error: {e}\")\n",
    "        return 1\n",
    "    \n",
    "setup_llama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    \"\"\"\n",
    "    Class to generate prompt template for Llama2-7b\n",
    "    \"\"\"\n",
    "    system_prompt = None\n",
    "    user_messages = []\n",
    "    model_replies = []\n",
    "\n",
    "    def __init__(self, system_prompt=None):\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def add_user_message(self, message: str, return_prompt=True):\n",
    "        self.user_messages.append(message)\n",
    "        if return_prompt:\n",
    "            return self.build_prompt()\n",
    "\n",
    "    def add_model_reply(self, reply: str, includes_history=True, return_reply=True):\n",
    "        reply_ = reply.replace(self.build_prompt(), \"\") if includes_history else reply\n",
    "        self.model_replies.append(reply_)\n",
    "        if len(self.user_messages) != len(self.model_replies):\n",
    "            raise ValueError(\n",
    "                \"Number of user messages does not equal number of system replies.\"\n",
    "            )\n",
    "        if return_reply:\n",
    "            return reply_\n",
    "\n",
    "    def get_user_messages(self, strip=True):\n",
    "        return [x.strip() for x in self.user_messages] if strip else self.user_messages\n",
    "\n",
    "    def get_model_replies(self, strip=True):\n",
    "        return [x.strip() for x in self.model_replies] if strip else self.model_replies\n",
    "\n",
    "    def clear_chat_history(self):\n",
    "        self.user_messages.clear()\n",
    "        self.model_replies.clear()\n",
    "\n",
    "    def build_prompt(self):\n",
    "        if self.user_messages == [] and self.model_replies == []:\n",
    "            return f\"<s>[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>> [/INST]</s>\"\n",
    "        \n",
    "        elif len(self.user_messages) != len(self.model_replies) + 1:\n",
    "            raise ValueError(\n",
    "                \"Error: Expected len(user_messages) = len(model_replies) + 1. Add a new user message!\"\n",
    "            )\n",
    "\n",
    "        if self.system_prompt is not None:\n",
    "            SYS = f\"[INST] <<SYS>>\\n{self.system_prompt}\\n<</SYS>>\"\n",
    "        else:\n",
    "            SYS = \"\"\n",
    "\n",
    "        CONVO = \"\"\n",
    "        SYS = \"<s>\" + SYS\n",
    "        for i in range(len(self.user_messages) - 1):\n",
    "            user_message, model_reply = self.user_messages[i], self.model_replies[i]\n",
    "            conversation_ = f\"{user_message} [/INST] {model_reply} </s>\"\n",
    "            if i != 0:\n",
    "                conversation_ = \"[INST] \" + conversation_\n",
    "            CONVO += conversation_\n",
    "\n",
    "        CONVO += f\"[INST] {self.user_messages[-1]} [/INST]\"\n",
    "\n",
    "        return SYS + CONVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel():\n",
    "    def __init__(self, mode:str, llama_model_path:str=None, llama_tokenizer_path:str=None):\n",
    "        \"\"\"\n",
    "        Llama Model Class for data cleaning\n",
    "\n",
    "        Args:\n",
    "            mode (str): Determines what type the LLama model should be, QnA or Classification\n",
    "            llama_model_path (str, optional): If locally hosting llama model, show the path of the model folder. Defaults to None.\n",
    "            llama_tokenizer_path (str, optional): If locally hosting llama model, show the path of the tokenizer folder. Defaults to None.        \n",
    "        \"\"\"\n",
    "        # Setup llama models\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llama_model, self.llama_tokenizer = self._load_llama(llama_model_path, llama_tokenizer_path)\n",
    "        self.config = GenerationConfig(max_new_tokens=1024,\n",
    "                        do_sample=True,\n",
    "                        top_k = 10,\n",
    "                        num_return_sequences = 1,\n",
    "                        return_full_text = False,\n",
    "                        temperature = 0.01,\n",
    "                        )\n",
    "\n",
    "        self.QAsystemPrompt = \"\"\"You are a question answering model, given a question and context in the format of a JSON:\n",
    "        {\n",
    "        \"question\":'',\n",
    "        \"context\":''\n",
    "        }\n",
    "        , you are to return the answer with the following JSON format.\n",
    "        {\n",
    "        \"answer\": \n",
    "        }\n",
    "        If the context does not answer the question, the answer is -.\n",
    "        Only return the JSON with the correct answer. No other text is allowed.\n",
    "        \"\"\"\n",
    "        # Note: Classification System Prompt is very specific to the leaf texture\n",
    "        self.ClassificationsystemPrompt = \"\"\"You are a classification model with 3 possible categories. Each category and it's description is given in the following JSON.\n",
    "        {\n",
    "        \"fine\": \"Linear, thin shaped leaves and stems with no spikes or rough edges. Leaves are needle like shape and should not stand upwards.\",\n",
    "        \"medium\": \"The most common texture in plants, medium size and shape. Often fleshy, rounded or oval, and not overly detailed. If unsure, pick this class.\",\n",
    "        \"coarse\": \"Must be large, broad leaves with size larger than 15cm. Often rough or thick with prominent veins, lobes or edges that stand out visually.\"\n",
    "        }\n",
    "        You will be given a description of the plant leaf type. Categorise the description into one of the classes and return your answer in following JSON format. Always prioritise the size before the shape texture in your classification.\n",
    "        {\n",
    "        \"answer\": class\n",
    "        }\n",
    "        Only return the JSON with the classification. No description is accepted. No other text.\n",
    "        \"\"\"\n",
    "        # Setup the correct system prompt based on the model requirements\n",
    "        self.promptGenerator = PromptTemplate(system_prompt= self.QAsystemPrompt if mode == 'QnA' else self.ClassificationsystemPrompt)\n",
    "\n",
    "\n",
    "    def _load_llama(self, llama_model_path:str, llama_tokenizer_path:str):\n",
    "        \"\"\"\n",
    "        Function to load llama 2-7b model\n",
    "        Uses hg_access from .env as a default, if there isn't any hg_access it looks for the directory of downloaded Llama models\n",
    "\n",
    "        Returns:\n",
    "            llama_model\n",
    "            llama_tokenizer\n",
    "        \"\"\"\n",
    "        hg_access = os.getenv('HG_ACCESS_TOKEN')\n",
    "        if hg_access != None:\n",
    "            try:\n",
    "                print(\"Loading Llama Models\")\n",
    "                llama_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "                llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=hg_access)\n",
    "                print(\"Llama Loaded Successfully\")\n",
    "                return llama_model, llama_tokenizer\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Unable to load Llama model from hugging face, reasons: {e}\")\n",
    "\n",
    "        elif llama_model_path != None and llama_tokenizer_path != None:\n",
    "            try:\n",
    "                print(\"Loading Llama Models\")\n",
    "                llama_model = AutoModelForCausalLM.from_pretrained(llama_model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "                llama_tokenizer = AutoTokenizer.from_pretrained(llama_tokenizer_path)\n",
    "                print(\"Llama Loaded Successfully\")\n",
    "                return llama_model, llama_tokenizer\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Unable to load Llama model from directory, reasons: {e}\")\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"No Llama resources provided\")\n",
    "\n",
    "\n",
    "    def question_answer(self, question:str, context:str):\n",
    "        \"\"\"\n",
    "        Function to call the llama model to question answer\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to be asked\n",
    "            context (str): The context that contains the answer to the question\n",
    "\n",
    "        Returns:\n",
    "            response (str): Llama2-7b model's response\n",
    "        \"\"\"\n",
    "        # Add user prompt\n",
    "        llama_prompt = self.promptGenerator.add_user_message(\n",
    "            json.dumps({\n",
    "                \"question\": question,\n",
    "                \"context\": context\n",
    "            })\n",
    "        )\n",
    "        # Generate response\n",
    "        encoded_input = self.llama_tokenizer.encode(llama_prompt, return_tensors='pt', add_special_tokens=False).to(self.device)\n",
    "        results = self.llama_model.generate(encoded_input, generation_config=self.config)\n",
    "        decoded_output = self.llama_tokenizer.decode(results[0], skip_special_tokens=True)\n",
    "        response = decoded_output.split(\"[/INST]\")[-1].strip()\n",
    "        self.promptGenerator.clear_chat_history() #Clear history to reset back to just system prompt\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "    def classify(self, context:str):\n",
    "        \"\"\"\n",
    "        Function to call the llama model to classify leaf texture\n",
    "\n",
    "        Args:\n",
    "            context (str): The context to help the model classify\n",
    "\n",
    "        Returns:\n",
    "            response (str): Llama2-7b model's response\n",
    "        \"\"\"\n",
    "        llama_prompt = self.promptGenerator.add_user_message(context)\n",
    "\n",
    "        encoded_input = self.llama_tokenizer.encode(llama_prompt, return_tensors='pt', add_special_tokens=False).to(self.device)\n",
    "        results = self.llama_model.generate(encoded_input, generation_config=self.config)\n",
    "        decoded_output = self.llama_tokenizer.decode(results[0], skip_special_tokens=True)\n",
    "        response = decoded_output.split(\"[/INST]\")[-1].strip()\n",
    "        self.promptGenerator.clear_chat_history()\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Things to check:\n",
    "1. Maximum height -> QA, extract tallest height from xx to xx, convert all to m and leave the integer\n",
    "2. Flower Colour -> QA, if flowers, spathe inside data, need to do QA\n",
    "3. Trunk Texture -> QA, if trunks/bark inside data\n",
    "4. Trunk Colour -> QA, if trunks/bark inside data\n",
    "5. Leaf Texture -> Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaningModel():\n",
    "    def __init__(self, csv_filepath:str=\"../src/flora_data/flora_species_updated.csv\"):\n",
    "        self.flora_data = pd.read_csv(csv_filepath)\n",
    "        # Updates to ensure all None is a string instead (if not it will be empty in the csv)\n",
    "        self.data = self.flora_data.where(pd.notnull(self.flora_data), 'None')\n",
    "\n",
    "\n",
    "    def clean_maximum_height(self, llama_model:AutoModelForCausalLM):\n",
    "        \"\"\"\n",
    "        Function to do QA on the maximum height and convert all heights to metre\n",
    "        Checks the data and if it does not follow a xx cm/m to xx cm/m format do QA\n",
    "\n",
    "        Args:\n",
    "            llama_model (AutoModelForCausalLM): Llama2-7b model set to QnA mode\n",
    "        \"\"\"\n",
    "        # Clear any chat history\n",
    "        llama_model.promptGenerator.clear_chat_history()\n",
    "        for index, value in tqdm(self.data['Maximum Height'].items(), total=len(self.data['Maximum Height']), desc=\"Cleaning Maximum Heights\"):\n",
    "            #Regex pattern for xx cm/m to xx cm/m\n",
    "            pattern = '^\\d+(\\.\\d+)?( ?(cm|m))?( to \\d+(\\.\\d+)?( ?(cm|m))?)?$'\n",
    "            if value != '-' and not re.match(pattern, value):\n",
    "                #Do not meet regex pattern and not - Llama QA\n",
    "                response = llama_model.question_answer(\"What is the max height of the plant in meters?\", value)\n",
    "                try:\n",
    "                    new_height = json.loads(response)['answer']\n",
    "                \n",
    "                except:\n",
    "                    # For when the model fails to return a JSON\n",
    "                    # Just assume no height for now\n",
    "                    new_height = '-'\n",
    "                \n",
    "                self.data.at[index, 'Maximum Height'] = new_height\n",
    "\n",
    "            # Meet the regex pattern, convert to metre\n",
    "            elif re.match(pattern, value):\n",
    "                max_height_m_int = value\n",
    "                max_height = value\n",
    "                if 'to' in value:\n",
    "                    max_height = value[value.index('to') + 2:] #Taking the latter half max height\n",
    "                \n",
    "                # Convert value from cm to metre\n",
    "                if 'cm' in max_height:\n",
    "                    max_height_cm_str = max_height[:(max_height.index('cm'))].strip() \n",
    "                    max_height_m_int = float(max_height_cm_str)/100\n",
    "                \n",
    "                elif 'm' in max_height:\n",
    "                    max_height_m_str = max_height[:max_height.index('m')].strip()\n",
    "                    max_height_m_int = float(max_height_m_str)\n",
    "\n",
    "                self.data.at[index, 'Maximum Height'] = max_height_m_int\n",
    "\n",
    "\n",
    "    def clean_flower_colour(self, llama_model:AutoModelForCausalLM):\n",
    "        \"\"\"\n",
    "        Function to do QA on the flower colour \n",
    "        Checks the data if flower, flowers, spathe are inside the text\n",
    "\n",
    "        Args:\n",
    "            llama_model (AutoModelForCausalLM): Llama2-7b model set to QnA mode\n",
    "        \"\"\"        \n",
    "        # Clear any chat history\n",
    "        llama_model.promptGenerator.clear_chat_history()\n",
    "        for index, value in tqdm(self.data['Flower Colour'].items(), total=len(self.data['Flower Colour']), desc='Cleaning Flower Colour'):\n",
    "            if 'flower' in value.lower() or 'flowers' in value.lower() or 'spathe' in value.lower():\n",
    "                #Llama QA\n",
    "                response = llama_model.question_answer(\"What are the colours of the flowers?\", value)\n",
    "                try:\n",
    "                    flower_colour = json.loads(response)['answer']\n",
    "                    # Just in case model returned a list \n",
    "                    try:\n",
    "                        flower_colour = ast.literal_eval(flower_colour)\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Just in case model returned a list \n",
    "                    if isinstance(flower_colour, list):\n",
    "                        flower_colour = \" \".join(flower_colour).title()\n",
    "                    # Captialise all words\n",
    "                    else:\n",
    "                       flower_colour = flower_colour.title()\n",
    "                \n",
    "                except:\n",
    "                    # For now, if fails js changed the height to - To be manully retrieved\n",
    "                    flower_colour = '-'\n",
    "\n",
    "                self.data.at[index, 'Flower Colour'] = flower_colour\n",
    "\n",
    "\n",
    "    def clean_trunk_texture(self, llama_model:AutoModelForCausalLM):\n",
    "        \"\"\"\n",
    "        Function to do QA on trunk texture\n",
    "        if trunk, trunks, bark, barks, stem, stems or girth in description, query\n",
    "\n",
    "        Args:\n",
    "            llama_model (AutoModelForCausalLM): Llama2-7b model set to QnA mode\n",
    "\n",
    "        \"\"\"\n",
    "        # Clear any chat history\n",
    "        llama_model.promptGenerator.clear_chat_history()\n",
    "        for index, value in tqdm(self.data['Trunk Texture'].items(), total=len(self.data['Trunk Texture']), desc='Cleaning Trunk Texture'):\n",
    "            response = None\n",
    "            if 'trunk' in value.lower() or 'trunks' in value.lower():\n",
    "                #Llama QA\n",
    "                response = llama_model.question_answer(\"What is the texture of the trunk?\", value)\n",
    "\n",
    "            elif 'bark' in value.lower() or 'barks' in value.lower():\n",
    "                response = llama_model.question_answer(\"What is the texture of the bark?\", value)\n",
    "\n",
    "            elif 'stem' in value.lower() or 'stems' in value.lower() or 'girth' in value.lower():\n",
    "                response = llama_model.question_answer(\"What is the texture of the stem?\", value)\n",
    "            # If any query was done\n",
    "            if response:\n",
    "                try:\n",
    "                    bark_texture = json.loads(response)['answer']\n",
    "                \n",
    "                except:\n",
    "                    bark_texture = '-'\n",
    "\n",
    "                self.data.at[index, 'Trunk Texture'] = bark_texture\n",
    "\n",
    "        \n",
    "    def clean_trunk_colour(self, llama_model:AutoModelForCausalLM):\n",
    "        \"\"\"\n",
    "        Function to do QA on trunk colour\n",
    "        if trunk, trunks, bark, barks, stem, stems or girth in description, query\n",
    "\n",
    "        Args:\n",
    "            llama_model (AutoModelForCausalLM): Llama2-7b model set to QnA mode\n",
    "\n",
    "        \"\"\"\n",
    "        # Clear any chat history\n",
    "        llama_model.promptGenerator.clear_chat_history()\n",
    "        for index, value in tqdm(self.data['Trunk Colour'].items(), total=len(self.data['Trunk Colour']), desc='Cleaning Trunk Colour'):\n",
    "            response = None\n",
    "            if 'trunk' in value.lower() or 'trunks' in value.lower():\n",
    "                #Llama QA\n",
    "                response = llama_model.question_answer(\"What is the colour of the trunk?\", value)\n",
    "\n",
    "            elif 'bark' in value.lower() or 'barks' in value.lower():\n",
    "                response = llama_model.question_answer(\"What is the colour of the bark?\", value)\n",
    "\n",
    "            elif 'stem' in value.lower() or 'stems' in value.lower() or 'girth' in value.lower():\n",
    "                response = llama_model.question_answer(\"What is the colour of the stem?\", value)\n",
    "            # If any query was done\n",
    "            if response:\n",
    "                try:\n",
    "                    bark_texture = json.loads(response)['answer']\n",
    "                \n",
    "                except:\n",
    "                    bark_texture = '-'\n",
    "\n",
    "                self.data.at[index, 'Trunk Colour'] = bark_texture\n",
    "    \n",
    "\n",
    "    def classify_leaf_texture(self, llama_model:AutoModelForCausalLM):\n",
    "        \"\"\"\n",
    "        Function to classify leaf texture\n",
    "\n",
    "        Args:\n",
    "            llama_model (AutoModelForCausalLM): Llama2-7b model set to Classification mode (preset for leaf texture)\n",
    "        \"\"\"\n",
    "        # Clear any chat history\n",
    "        llama_model.promptGenerator.clear_chat_history()\n",
    "        for index, value in tqdm(self.data['Leaf Texture'].items(), total=len(self.data['Leaf Texture']), desc='Classifying Leaf Texture'):\n",
    "            if value != '-' and value != 'None':\n",
    "                response = llama_model.classify(value)\n",
    "                try:\n",
    "                    leaf_texture = json.loads(response)['answer']\n",
    "                    if leaf_texture.lower() in ['fine', 'medium', 'coarse']:\n",
    "                        leaf_texture = leaf_texture.title()\n",
    "                    else:\n",
    "                        leaf_texture = '-'\n",
    "                \n",
    "                except:\n",
    "                    leaf_texture = '-'\n",
    "\n",
    "                self.data.at[index, 'Leaf Texture'] = leaf_texture\n",
    "\n",
    "\n",
    "    def clean_data(self, output_path:str):\n",
    "        \"\"\"\n",
    "        Function to run all data cleaning functions before downloading updated data into a csv file\n",
    "\n",
    "        Args:\n",
    "            output_path (str): filepath to csv output for clean dataset\n",
    "        \"\"\"\n",
    "        QnAModel = LlamaModel('QnA')\n",
    "        print(f\"Starting data cleaning.\")\n",
    "        self.clean_maximum_height(QnAModel)\n",
    "        self.clean_flower_colour(QnAModel)\n",
    "        self.clean_trunk_texture(QnAModel)\n",
    "        self.clean_trunk_colour(QnAModel)\n",
    "\n",
    "        print(\"Information extration complete, starting classification.\")\n",
    "        del QnAModel\n",
    "        gc.collect()\n",
    "        classificationModel = LlamaModel(\"Classification\")\n",
    "        self.classify_leaf_texture(classificationModel)\n",
    "\n",
    "        print(\"Data cleaning completed.\")\n",
    "        self.data.to_csv(output_path, index=False)\n",
    "        print(f\"Data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataCleaningModel(csv_filepath=\"../src/flora_data/flora_species_updated.csv\")\n",
    "dataset.clean_data('../src/flora_data/cleaned_flora_species.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
