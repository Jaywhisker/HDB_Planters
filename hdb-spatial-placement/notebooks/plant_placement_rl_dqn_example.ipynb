{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plant Placement RL (Deep Q Learning Example)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a example usage of Tianshou's RL library which makes use of Deep Q learning, an off policy RL technique to train a very simplistic version of a plant placement model. There are 3 different plants types and no characteristics have been encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Necessary Libararies (Use Python 3.11 Please)\n",
    "\n",
    "1. gymnasium\n",
    "2. numpy\n",
    "3. torch\n",
    "4. tianshou (latest version) Note: If you pip install tianshou, it will not give you the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/thu-ml/tianshou.git@master --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Import Necessary Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.data import Collector, VectorReplayBuffer, Batch\n",
    "from tianshou.trainer import OffpolicyTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Define Plant Env Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.grid_size = (5, 5)  # 5x5 grid\n",
    "        self.possible_plants = [1, 2, 3]  # different types of plants\n",
    "        self.num_actions = self.grid_size[0] * self.grid_size[1] * len(self.possible_plants)  # total action count\n",
    "        self.max_steps = 10  # Maximum steps per episode\n",
    "        self.current_step = 0  # Track the number of steps taken\n",
    "        \n",
    "        # Define state and action spaces\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=3, shape=self.grid_size, dtype=int)\n",
    "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        # Reset the board to empty\n",
    "        self.state = np.zeros(self.grid_size, dtype=int)\n",
    "        self.current_step = 0\n",
    "        return self.state.flatten(), {}  # Return observation and empty info dict\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Unflatten the action\n",
    "        x = (action // len(self.possible_plants)) // self.grid_size[1]\n",
    "        y = (action // len(self.possible_plants)) % self.grid_size[1]\n",
    "        plant_type = action % len(self.possible_plants) + 1  # Plant type starts at 1\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Place the plant and calculate reward\n",
    "        if self.state[x, y] == 0:  # if empty\n",
    "            self.state[x, y] = plant_type\n",
    "            reward += 1  # +1 for a valid placement\n",
    "            \n",
    "            # Additional rewards or penalties based on neighbors\n",
    "            reward += self.calculate_spacing_reward(x, y, plant_type)\n",
    "        else:\n",
    "            reward -= 1  # -1 for invalid placement on occupied space\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps  # End episode after max_steps\n",
    "        \n",
    "        return self.state.flatten(), reward, done, False, {}  # Return empty info dict\n",
    "\n",
    "    def calculate_spacing_reward(self, x, y, plant_type):\n",
    "        \"\"\"Reward based on spacing and neighboring plants.\"\"\"\n",
    "        reward = 0\n",
    "        # Penalize for placing the same plant type too close\n",
    "        neighbors = [\n",
    "            (x - 1, y), (x + 1, y),  # left and right\n",
    "            (x, y - 1), (x, y + 1),  # up and down\n",
    "        ]\n",
    "        for nx, ny in neighbors:\n",
    "            if 0 <= nx < self.grid_size[0] and 0 <= ny < self.grid_size[1]:  # Check boundaries\n",
    "                if self.state[nx, ny] == plant_type:\n",
    "                    reward -= 0.5  # Penalize for placing similar plants adjacent\n",
    "                elif self.state[nx, ny] != 0:\n",
    "                    reward += 0.2  # Reward for diversity in neighboring plants\n",
    "        return reward\n",
    "\n",
    "    def render(self):\n",
    "        print(self.state)  # Print grid to visualize the placement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Define Neural Netowrk and RL Policy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network that learns the rules\n",
    "state_shape = (5 * 5,)  # Flattened grid shape\n",
    "num_actions = 5 * 5 * 3  # Total number of possible actions\n",
    "\n",
    "net = Net(\n",
    "    state_shape=state_shape, \n",
    "    action_shape=num_actions,  # Flattened action space\n",
    "    hidden_sizes=[64, 64], \n",
    "    device='cpu'\n",
    ")\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Create a policy for the agent using DQN\n",
    "policy = DQNPolicy(\n",
    "    model=net, \n",
    "    optim=optim, \n",
    "    discount_factor=0.9, \n",
    "    estimation_step=1, \n",
    "    target_update_freq=100,\n",
    "    action_space=gym.spaces.Discrete(num_actions)  # Specify action space\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Create Environment and Buffers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment for training\n",
    "train_env = PlantEnvironment()\n",
    "test_env = PlantEnvironment()\n",
    "\n",
    "# Create buffer for the robot's memory\n",
    "train_buffer = VectorReplayBuffer(total_size=10000, buffer_num=1)\n",
    "\n",
    "# Create collectors to collect experiences while training and testing\n",
    "train_collector = Collector(policy, train_env, train_buffer)\n",
    "test_collector = Collector(policy, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Define Trainer and Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training\n",
    "trainer = OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,  \n",
    "    step_per_epoch=1000,  \n",
    "    step_per_collect=10, \n",
    "    episode_per_test=10,  # test every 10 episodes\n",
    "    batch_size=64,  \n",
    "    update_per_step=0.1,  \n",
    ")\n",
    "\n",
    "# Start training\n",
    "result = trainer.run()\n",
    "print(f\"Training finished! Best reward: {result.best_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = test_env.reset()\n",
    "done = False\n",
    "\n",
    "# Test the trained policy\n",
    "while not done:\n",
    "    # Wrap the state in a Batch object with an empty info dictionary\n",
    "    action = policy.forward(Batch(obs=[state], info={})).act[0]\n",
    "    state, reward, done, _, _ = test_env.step(action)\n",
    "    test_env.render()  # visualize grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
